# LLM Coding Capability Assessment Framework

This repository contains a structured framework for evaluating Large Language Models' (LLMs) coding capabilities through a series of carefully designed programming challenges.

## Research Methodology

### Test Structure
Each test in this framework follows a consistent structure:
- A `prompt.txt` file containing the problem specification
- A `benchmarking.py` file implementing test cases and verification
- Solution files generated by LLMs being evaluated

### Data Collection Process

1. **Problem Assignment**
   - Contributors provide the content of `prompt.txt` to their assigned LLM/code assistant
   - The problem specifications are designed to test specific algorithmic and implementation capabilities
   - Each prompt includes clear input/output formats and constraints

2. **Solution Generation**
   - The LLM generates a solution based on the prompt
   - If the LLM attempts to create an application or provides a non-extractable solution, contributors may refine the prompt to ensure a clear, testable output
   - The core challenge and expected output format must remain unchanged during any prompt refinement

3. **Solution Extraction**
   - Contributors extract the LLM's solution into a Python file
   - The solution must be taken as-is from the LLM's output
   - No modifications or optimizations are allowed to the LLM's code
   - If the LLM provides multiple solutions, only the final/complete version should be used

4. **Verification**
   - Solutions are tested using the corresponding `benchmarking.py`
   - Test cases cover various edge cases and performance requirements
   - Results are automatically scored and documented

### Key Principles

1. **Consistency with Adaptability**
   - Base problems and evaluation criteria remain constant across all LLMs
   - Prompt engineering may be necessary for different LLMs:
     * Some LLMs may attempt to create full applications instead of focused solutions
     * Others may require specific guidance to produce extractable code
     * Contributors should document any prompt modifications needed
   - Testing infrastructure and scoring remain standardized regardless of prompt adaptations

2. **Non-Intervention**
   - Contributors must not modify or optimize the LLM's code
   - The goal is to assess raw LLM output, not human-assisted solutions
   - Any necessary clarifications should be made through prompt refinement only

3. **Transparency**
   - All prompts, test cases, and evaluation criteria are publicly available
   - The testing process is documented and reproducible
   - Results can be independently verified
   - All prompt modifications must be documented and justified

4. **Fairness**
   - Problems are designed to be solvable within LLM context windows
   - Time and memory constraints are reasonable for the given tasks
   - Edge cases are documented in the prompts
   - Prompt refinements must not simplify the core challenge

## Repository Structure

```
.
├── README.md
├── strongly_connected_components/
│   ├── prompt.txt
│   └── benchmarking.py
├── reduced_row_echelon/
│   ├── prompt.txt
│   └── benchmarking.py
├── sat_solver/
│   ├── prompt.txt
│   └── benchmarking.py
├── balanced_brackets/
│   ├── prompt.txt
│   └── benchmarking.py
├── hash_calculator/
│   ├── prompt.txt
│   └── benchmarking.py
└── primality/
    ├── prompt.txt
    └── benchmarking.py
```

## Usage for Contributors

1. Clone this repository
2. Select a test problem directory
3. Provide the contents of `prompt.txt` to your assigned LLM/code assistant
4. Extract the LLM's solution into a new Python file in the same directory
5. Run the benchmarking script to evaluate the solution:
   ```bash
   python benchmarking.py solution.py
   ```

## Contributing

This framework is designed for research purposes. When contributing:
- Maintain the existing problem structure
- Do not modify existing test cases or evaluation criteria
- Document any prompt refinements needed for different LLMs
- Share insights about prompt engineering that led to extractable solutions

## Research Integrity

The goal of this framework is to provide objective, reproducible assessments of LLM coding capabilities. Contributors should:
- Report all prompt refinements used to obtain solutions
- Document any limitations or failures encountered
- Maintain the integrity of the LLM's original output
- Share both successful and unsuccessful evaluation attempts

## License

This framework is provided for research purposes. Please check individual licenses for any LLMs used in testing. 